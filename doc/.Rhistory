avg.g[avg.g$Year==1972,]
avg.g[avg.g$Year==1989,]
year.avgs<- ddply(debt, .(Year), m.rate)
year.avgs[year.avgs$Year==1972,]
year.avgs[year.avgs$Year==1989,]
year.avgs<- ddply(debt, .(Year), m.rate)
colnames(year.avgs)<-c('Year','AverageGrowth')
year.avgs[year.avgs$Year==1972,]
year.avgs[year.avgs$Year==1989,]
year.avgs.new<-year.avgs[year.avgs$Year>=1946,]
year.avgs.new
year.avgs.new.sort<-year.avgs.new[order(year.avgs.new$Year),]
year.avgs.new.sort
year.avgs.new.sortrate<-year.avgs.new.sort[order(
year.avgs.new.sort$AverageGrowth,decreasing = T),]
year.avgs.new.sortrate
year.avgs.new.sortrate
x<-as.numeric(year.avgs$Year)
y<-as.numeric(year.avgs$AverageGrowth)
plot(x,y,ylab = "growth rates", xlab = "year ")
cor(debt$growth,debt$ratio)
cor_g.r<-cor(debt$growth,debt$ratio)
cor_g.r
cor_g.r<-round(cor_g.r,digits=2)
cor_g.r
cor_g.r<-round(cor_g.r,digits=3)
cor_g.r
cor_g.r<-cor(debt$growth,debt$ratio)
cor_g.r
cor_g.r<-round(cor_g.r,digits=4)
cor_g.r
split.year
sapply(split.year,cor.y)
cor.y<-function(df){
return(round(cor(df$growth,df$ratio),digits=4))
}
sapply(split.year,cor.y)
mean(sapply(split.year,cor.y))
round(mean(sapply(split.year,cor.y)),digits=4)
cor.y<-function(df){
return(cor(df$growth,df$ratio))
}
round(mean(sapply(split.year,cor.y)),digits=4)
debt.f<-debt[debt$Country=='France',]
debt.f
dim(debt.f)
n.growth <- function(year, country.df) {
if(any(country.df$Year == (year + 1))) {
return(country.df$growth[country.df$Year == (year + 1)])
} else {
return(NA)
}
}
debt.f$n.growth <- sapply(debt.f$Year, n.growth, debt.f) debt.fr$next.growth[debt.fr$Year == 1971]
n.growth <- function(year, country.df) {
if(any(country.df$Year == (year + 1))) {
return(country.df$growth[country.df$Year == (year + 1)])
} else {
return(NA)
}
}
debt.f$n.growth <- sapply(debt.f$Year, n.growth, debt.f) debt.fr$next.growth[debt.fr$Year == 1971]
debt.f$n.growth <- sapply(debt.f$Year, n.growth, debt.f)
debt.f$n.growth[debt.f$Year == 1971]
round(debt.f$n.growth[debt.f$Year == 1971],digits=4)
round(debt.f$n.growth[debt.f$Year == 1971],digits=3)
debt.f$n.growth[debt.f$Year == 1972]
round(debt.f$n.growth[debt.f$Year == 1971],digits=3)
debt.f$n.growth[debt.f$Year == 1972]
next.growth.all <- function(country.df) {
country.df$n.growth <- sapply(country.df$Year, n.growth, country.df)
return(country.df)
}
debt <- ddply(debt, .(Country), next.growth.all) debt$next.growth[debt$Country == "France" & debt$Year == 2009]
debt <- ddply(debt, .(Country), next.growth.all)
debt$next.growth[debt$Country == "France" & debt$Year == 2009]
next.growth.all <- function(country.df) {
country.df$next.growth <- sapply(country.df$Year, next.growth, country.df)
return(country.df)
}
debt <- ddply(debt, .(Country), next.growth.all)
next.growth.all <- function(country.df) {
country.df$next.growth <- sapply(country.df$Year, next.growth, country.df)
return(country.df)
}
debt <- ddply(debt, .(Country), next.growth.all)
next.growth.all <- function(country.df) {
country.df$next.growth <- sapply(country.df$Year, n.growth, country.df)
return(country.df)
}
debt <- ddply(debt, .(Country), next.growth.all)
debt$next.growth[debt$Country == "France" & debt$Year == 2009]
?rnorm
rbinom<-(400,1/3)
rbinom<-(1,400,1/3)
rbinom(1,400,1/3)
rbinom(400,1/3)
rbinom(400,1,1/3)
data<-rbinom(400,1000,1/3)
data
data[data<=150]
which(data<=150)
data<=150
data[data<=150]
data <- rnorm(1000, 133, 9)
data<-rbinom(1000,400,1/3)
data
data[data<=100 & data>=150]
data[data<=100 | data>=150]
length(data[data<=100 | data>=150])
length(data)
length(data[data<=100 | data>=150])/length(data)
data[data<=100 | data>=150]
data<=100
data[data<=100]
length(data[data<=100 | data>=150])/length(data)
prop<-length(data[data<=100 | data>=150])/length(data)
data2<-rnorm(1000,400/3,400/3*(2/3))
length(data[data2<=100 | data2>=150])
prop2<-length(data[data2<=100 | data2>=150])/length(data2)
prop
prop2
x <- rbinom(1000, size = 400, prob = 1/3)
data2<-rnorm(1000,400/3,sqrt(400/3*(2/3)))
prop2<-length(data[data2<=100 | data2>=150])/length(data2)
prop2
x <- rbinom(1000, size = 400, prob = 1/3)
ggplot(data = data.frame(x)) +
geom_histogram(aes(x = x, y = ..density..)) +
stat_function(mapping = aes(x = x), fun = dnorm,
args = list(mean = 400/3, sd = sqrt(400/3*(2/3))), color = "red") +
labs(title = "Normal Approximation to the Binomial")
rbinom(400,1,1/3)
rbinom(1,400,1/3)
data<-rbinom(1000,400,1/3)
MomentEstimator<-function(data,n){
phat<-data/n
return (phat)
}
MomentEstimator(data,400)
phat<-mean(data/n)
MomentEstimator(data,400)
MomentEstimator<-function(data,n){
phat<-mean(data/n)
return (phat)
}
MomentEstimator(data,400)
MomentEstimator(80,100)
bin<-function(para,data){
return(-sum(dbinom(data, para, log = TRUE)))
}
nlm(bin, 0.5, data = data)[1:3]
bin<-function(para,data){
para<-para
return(-sum(dbinom(data, para, log = TRUE)))
}
nlm(bin, 0.5, data = data)[1:3]
bin(0.5,data)
bin<-function(para,data){
para<-para
return(-sum(dbinom(data, prob=para, log = TRUE)))
}
bin(0.5,data)
nlm(bin, c(1, 0.5), data = data)
nlm(bin, c(1000, 0.5), data = data)
bin<-function(para,data){
n<-para[1]
p<-para[2]
return(-sum(dbinom(data, size=n ,prob=para, log = TRUE)))
}
nlm(bin, c(1000, 0.5), data = data)
B<-5000
param_ests<-matrix(NA, nrow = B, ncol=1)
colnames(param_ests) <- c("p"")
for (b in 1:B) {
resamp<- sample(1:n, n, replace = TRUE)
param_ests[b, ]<- gam.MMest(cats$Hwt[resamp])
}
# Your answer to question 1.g here.  Don't remove the set.seed(3) command.
colnames(param_ests) <- c("p")
B<-5000
param_ests<-matrix(NA, nrow = B, ncol=1)
colnames(param_ests) <- c("p")
B<-5000
n<-length(data)
param_ests<-c()
B<-5000
n<-length(data)
param_ests<-c()
for (b in 1:B) {
resamp<- sample(1:n, n, replace = TRUE)
param_ests[b]<- mean(resamp)
}
mean(param_ests)
ggplot() + geom_histogram(aes(x = param_ests))
set.seed(1)
rbinom(1,400,1/3)
set.seed(2)
data<-rbinom(1000,400,1/3)
x <- rbinom(1000, size = 400, prob = 1/3)
ggplot(data = data.frame(x)) +
geom_histogram(aes(x = x, y = ..density..)) +
stat_function(mapping = aes(x = x), fun = dnorm,
args = list(mean = 400/3, sd = sqrt(400/3*(2/3))), color = "red") +
labs(title = "Normal Approximation to the Binomial")
prop<-length(data[data<=100 | data>=150])/length(data)
prop
data2<-rnorm(1000,400/3,sqrt(400/3*(2/3)))
prop2<-length(data[data2<=100 | data2>=150])/length(data2)
prop2
prop
prop2
MomentEstimator<-function(data,n){
phat<-mean(data/n)
return (phat)
}
MomentEstimator(data,400)
MomentEstimator(80,100)
bin<-function(para,data){
n<-para[1]
p<-para[2]
return(-sum(dbinom(data, size=n ,prob=para, log = TRUE)))
}
nlm(bin, c(1000, 0.5), data = data)
set.seed(3)
B<-5000
n<-length(data)
param_ests<-c()
for (b in 1:B) {
resamp<- sample(1:n, n, replace = TRUE)
param_ests[b]<- mean(resamp)
}
mean(param_ests)
5.56e − 07
5.56e−07
mean(param_ests)
ggplot() + geom_histogram(aes(x = param_ests))
mean(data)
mmean_est1 <- mean(samp1)
mmean_est1
samp_size<-100
sample1<-rnorm(samp_size, mean = 10, sd = 3)
mmean_est1 <- mean(samp1)
mmean_est1
B<-1000
param_ests1<-c()
for (b in 1:B){
resamp<-sample(sample1,samp_size,replace=T)
param_ests1[b]<- mean(resamp)
}
mean(param_ests1)
data<-rbinom(1000,400,1/3)
mean(data)
ggplot() + geom_histogram(aes(x = param_ests))
debt <- read.csv("debt.csv", as.is = TRUE)
dim(debt)
head(debt)
avg.growth['1972',]
avg.growth['1989',]
year.avgs[year.avgs$Year==1972,]
year.avgs[year.avgs$Year==1989,]
year.avgs.new.sortrate
plot(x,y,ylab = "growth rates", xlab = "year ")
x<-as.numeric(year.avgs$Year)
y<-as.numeric(year.avgs$AverageGrowth)
plot(x,y,ylab = "growth rates", xlab = "year ")
round(mean(sapply(split.year,cor.y)),digits=4)
dim(debt.f)
debt.f$n.growth <- sapply(debt.f$Year, n.growth, debt.f)
round(debt.f$n.growth[debt.f$Year == 1971],digits=3)
debt.f$n.growth[debt.f$Year == 1972]
debt$next.growth[debt$Country == "France" & debt$Year == 2009]
next.growth.all <- function(country.df) {
country.df$next.growth <- sapply(country.df$Year, n.growth, country.df)
return(country.df)
}
debt <- ddply(debt, .(Country), next.growth.all)
debt$next.growth[debt$Country == "France" & debt$Year == 2009]
library(ggplot2)
library(plyr)
library('tidyverse')
install.packages('tidyverse')
library('tidyverse')
library(tidyverse)
library(dplyr)
library('dplyr')
library(dplyr)
df<-data.frame(x=rnorm(100))
hist(df$x)
library(ggplot2)
install.packages('tidyverse')
install.packages("tidyverse")
library(tidyverse)
library(dplyr)
install.packages('umnsell')
install.packages('munsell')
library(dplyr)
install.packages('dplyr')
library(dplyr)
install.packages('dplyr')
library(dplyr)
install.packages('tidyverse')
install.packages("tidyverse")
library(tidyverse)
?tidyverse
head(spooky)
packages.used <- c("ggplot2", "dplyr", "tidytext", "wordcloud", "stringr", "ggridges")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
library(ggplot2)
library(dplyr)
library(tidytext)
library(wordcloud)
head(spooky)
head(spooky)
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
head(spooky)
head(spooky_wrd)
spooky_wrd <- unnest_tokens(spooky, word, text)
head(spooky_wrd)
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
head(stop_words)
tail(stop_words)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
head(spooky_wrd)
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
# install additional packages
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
install.packages(packages.needed, dependencies = TRUE, repos = "http://cran.us.r-project.org")
source("../libs/multiplot.R")
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
head(spooky)
dim(spooky)
spooky[,1]
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
spooky_wrd <- unnest_tokens(spooky, word, text)
library(tidytext)
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd
dim(stop_words)
head(stop_words)
tail(stop_words)
unique(stop_words$lexicon)
?stop_words
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n
library(dplyr)
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n
head(words)
tail(words)
head(freqs)
tail(freqs)
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
library(ggplot2)
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
?str_length
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
library(stringr)
library(ggplot2)
library(stringr)
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
?geom_density_ridges
??geom_density_ridges
library(ggridges)
library(ggplot2)
library(stringr)
library(ggridges)
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
spooky_wrd$word_length <- str_length(spooky_wrd$word)
head(spooky_wrd$word_length)
p3 <- ggplot(spooky_wrd) +
geom_density(aes(word_length, fill = author), bw = 0.05, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Word length [# characters]")
layout <- matrix(c(1, 2, 1, 3), 2, 2, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
frequency <- count(spooky_wrd, author, word)
tf_idf    <- bind_tf_idf(frequency, word, author, n)
tf_idf
head(tf_idf)
# how important is a word for single author compared to entire document
frequency <- count(spooky_wrd, author, word)
#calculate frequency for single author relative to all the user use taht word
tf_idf    <- bind_tf_idf(frequency, word, author, n)
head(tf_idf)
tail(tf_idf)
tf_idf    <- arrange(tf_idf, desc(tf_idf))
tf_idf    <- mutate(tf_idf, word = factor(word, levels = rev(unique(word))))
# Grab the top thirty tf_idf scores in all the words
tf_idf_30 <- top_n(tf_idf, 30, tf_idf)
ggplot(tf_idf_30) +
geom_col(aes(word, tf_idf, fill = author)) +
labs(x = NULL, y = "TF-IDF values") +
theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))
# Grab the top twenty tf_idf scores in all the words for each author
tf_idf <- ungroup(top_n(group_by(tf_idf, author), 20, tf_idf))
ggplot(tf_idf) +
geom_col(aes(word, tf_idf, fill = author)) +
labs(x = NULL, y = "tf-idf") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
get_sentiments('nrc')
# Keep words that have been classified within the NRC lexicon.
get_sentiments('nrc')
#drop words from spooky_word which does not have sentiments
sentiments <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")
nrc_pos <- filter(get_sentiments('nrc'), sentiment == "positive")
nrc_pos
positive <- inner_join(spooky_wrd, nrc_pos, by = "word")
head(positive)
count(positive, word, sort = TRUE)
pos_words     <- count(group_by(positive, word, author))
pos_words_all <- count(group_by(positive, word))
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words <- arrange(pos_words, desc(n.y))
pos_words <- ungroup(head(pos_words, 81))
# Note the above is the same as
# pos_words <- pos_words  %>%
#                left_join(pos_words_all, by = "word") %>%
#                arrange(desc(n.y)) %>%
#                head(81) %>%
#                ungroup()
ggplot(pos_words) +
geom_col(aes(reorder(word, n.y, FUN = min), n.x, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
# Counts how many times each word appears in each sentence
sent_wrd_freqs <- count(spooky_wrd, id, word)
head(sent_wrd_freqs)
# Creates a DTM matrix
spooky_wrd_tm <- cast_dtm(sent_wrd_freqs, id, word, n)
?slam
??slam
install.packages('slam')
setwd("~/Desktop/github/spring2018-project1-changxu33/doc")
setwd("~/")
setwd("~/Desktop/github/spring2018-project1-changxu33/doc")
