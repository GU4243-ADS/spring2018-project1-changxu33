---
title: "Project 1_App_ds"
author: "Cindy Xu UNI:cx2199"
date: "1/31/2018"
output: pdf_document
---
# Section 1: Check and install needed packages. Load the libraries and functions. 
```{r, message = F, warning = F}
packages.used<-c("ggplot2","dplyr","tibble","tidyr","stringr", "tidytext","topicmodels","wordcloud","ggridges")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}

library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
library(RColorBrewer)
source("../libs/multiplot.R")
```

#Section 2:Read in the data
The following code assumes that the dataset `spooky.csv` lives in a `data` folder (and that we are inside a `doc` folder).

## Step 1: Using spooky 
```{r}
spooky<-read.csv('../data/spooky.csv',as.is=T)
```

## An overview of the data structure and content
Let's first remind ourselves of the structure of the data.

```{r}
dim<-dim(spooky)
dim
head(spooky)
summary(spooky)
sum(is.na(spooky))
spooky$author<-as.factor(spooky$author)
unique(spooky$author)
```

When we look into spooky data set, it is a 19579 rows and 3 columns dataset. Each row correspoding a unique id number, an excerpt of texts, and author name. Addtionally, there are no missing values. There are three authors, Like `HPL` is Lovecraft, `MWS` is Shelly, and `EAP` is Poe.  

##Step 5: data Processing
Punctuation -- typical sentence structure.  Clauses they have.  Number of commas or semicolons.

```{r}
str_count(spooky,',')
str_count(spooky,';' )

```
Poe used commas 19578 times, Lovecraft used commas 57798 times, Shelly used commas 19578 times. Poe used semicolons 0 times, Lovecraft used semicolons 5159 times, Shelly used semicolons 0 times. clause

Dialog
```{r}

```

He/she
```{r}
str_count(spooky,'He')
str_count(spooky,'he')
str_count(spooky,'She')
str_count(spooky,'she')
```
Poe and Shelly did not use he/she, Lovecraft used he more than she.

Drop all punctuation and transform all words into lower case.
```{r}
spooky_wrd<-unnest_tokens(spooky,word,text)
```

Bi-grams, n-grams
```{r}
# Make a table with one word per row and remove `stop words` (i.e. the common words).
bigrams<-unnest_tokens(spooky,bigram, text, token = "ngrams", n = 2)
```
By seeing how often word X is followed by word Y, we can then build a model of the relationships between them. 
This data structure is still a variation of the tidy text format. It is structured as one-token-per-row (with extra metadata, such as author, still preserved), but each token now represents a bigram.

Counting and filtering n-grams

Our usual tidy tools apply equally well to n-gram analysis. We can examine the most common bigrams using dplyr’s count() to customize stop-words
```{r}
count(bigrams,bigram, sort=T)
```

As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and in the: what we call “stop-words” . This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.
```{r}
bigrams_separated<-separate(bigrams,bigram,c("word1", "word2"),sep = " ")
bigrams_filtered<-bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts<-bigrams_filtered %>% 
  count(word1,word2,sort=T)
bigram_counts
```

We can see that these phrases are the most common pairs in spooky data set.

In other analyses, we may want to work with the recombined words. tidyr’s unite() function is the inverse of separate(), and lets us recombine the columns into one. Thus, “separate/filter/count/unite” let us find the most common bigrams not containing stop-words.

```{r}
bigrams_united<-bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigrams_united
```

Analyzing bigrams

A bigram can also be treated as a term in a document in the same way that we treated individual words. 

First, get rid of stop-words. And find word frequency

```{r}
# Words is a list of words, and freqs their frequencies
spooky_wrd<-anti_join(spooky_wrd,stop_words,by="word")
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n
head(sort(freqs, decreasing = TRUE))

png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 10, color = c("purple4", "red4", "black"))
dev.off()
```

We can compare the way the authors use the most frequent words too.

```{r}
# Counts number of times each author used each word.
author_words<-count(group_by(spooky_wrd, word, author))

# Counts number of times each word was used.
all_words<-rename(count(group_by(spooky_wrd,word)),all = n)

author_words<-left_join(author_words,all_words,by="word")
author_words<-arrange(author_words,desc(all))
author_words<-ungroup(head(author_words,81))
  
ggplot(author_words) +
  geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")
```

We'll do some simple numerical summaries of the data to provide some nice visualizations.

```{r, message = FALSE}
p1 <- ggplot(spooky) +
      geom_bar(aes(author, fill = author)) +
      theme(legend.position = "none")


spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)

p2 <- ggplot(spooky) +
      geom_density_ridges(aes(sen_length, author, fill = author)) +
      scale_x_log10() +
      theme(legend.position = "none") +
      labs(x = "Sentence length [# characters]")


spooky_wrd$word_length <- str_length(spooky_wrd$word)
head(spooky_wrd$word_length)

p3 <- ggplot(spooky_wrd) +
      geom_density(aes(word_length, fill = author), bw = 0.05, alpha = 0.3) +
      scale_x_log10() +
      theme(legend.position = "none") +
      labs(x = "Word length [# characters]")

layout <- matrix(c(1, 2, 1, 3), 2, 2, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
```
From the above plots we find:

* EAP is featured most frequently.

* Sentence length for EAP is more variable.

Then get tf_dif
```{r}
frequency<-count(spooky_wrd,author,word)
tf_idf<-bind_tf_idf(frequency,word,author,n)
tf_idf<-arrange(tf_idf, desc(tf_idf))
tf_idf<-mutate(tf_idf, word = factor(word,levels =
                                       rev(unique(word))))
# Grab the top fourty tf_idf scores in all the words 
tf_idf_40<-top_n(tf_idf,40,tf_idf)

ggplot(tf_idf_40) +
  geom_col(aes(word, tf_idf, fill = author)) +
  labs(x = NULL, y = "TF-IDF values") +
  theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))
```
Note that in the above, many of the words recognized by their tf-idf scores are names.  This makes sense -- if we see text referencing Raymond, Idris, or Perdita, we know almost for sure that MWS is the author.  But some non-names stand out.  EAP often uses "monsieur" and "jupiter" while HPL uses the words "bearded" and "attic" more frequently than the others.  We can also look at the most characteristic terms per author.

```{r}
# Grab the top thirty tf_idf scores in all the words for each author
tf_idf<-ungroup(top_n(group_by(tf_idf, author),30,tf_idf))
  
ggplot(tf_idf_30) +
  geom_col(aes(word, tf_idf, fill = author)) +
  labs(x = NULL, y = "tf-idf") +
  theme(legend.position = "none") +
  facet_wrap(~ author, ncol = 3, scales = "free") +
  coord_flip() +
  labs(y = "TF-IDF values")
```

Then we can look at the tf-idf of bigrams across spooky datasts. These tf-idf values can be visualized within each author.
```{r}
bigram_tf_idf<-bigrams_united %>%
  count(author,bigram) %>%
  bind_tf_idf(bigram,author,n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf
```

Using bigrams to provide context in sentiment analysis

Now that we have the data organized into bigrams, it’s easy to tell how often words are preceded by a word like “not”:
```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)
```
By performing sentiment analysis on the bigram data, we can examine how often sentiment-associated words are preceded by “not” or other negating words. We could use this to ignore or even reverse their contribution to the sentiment score.

Let’s use the AFINN lexicon for sentiment analysis, which you may recall gives a numeric sentiment score for each word, with positive or negative numbers indicating the direction of the sentiment.

```{r}
AFINN<-get_sentiments("afinn")
AFINN
```

We can then examine the most frequent words that were preceded by “not” and were associated with a sentiment.

```{r}
not_words<-bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()
not_words
```
For example, the most common sentiment-associated word to follow “not” was “help”, which would normally have a (positive) score of 2.

It’s worth asking which words contributed the most in the “wrong” direction. To compute that, we can multiply their score by the number of times they appear (so that a word with a score of +3 occurring 10 times has as much impact as a word with a sentiment score of +1 occurring 30 times). We visualize the result with a bar plot.

```{r}
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
```
The 20 words preceded by ‘not’ that had the greatest contribution to sentiment scores, in either a positive or negative direction

he bigrams “not help” and “not like” were overwhelmingly the largest causes of misidentification, making the text seem much more positive than it is. But we can see phrases like “not fail” and “not die” sometimes suggest text is more negative than it is.

Not” isn’t the only term that provides some context for the following word. We could pick four common words (or more) that negate the subsequent term, and use the same joining and counting approach to examine all of them at once.
```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()
negated_words
```

“not doubt” and “not help” are  the two most common examples, 
we can also see pairings such as “no hpe” and “never forget.” We could combine thisto reverse the AFINN scores of each word that follows a negation. 

Visualizing a network of bigrams with ggraph

We may be interested in visualizing all of the relationships among words simultaneously, rather than just the top few at a time. As one common visualization, we can arrange the words into a network, or “graph.” Here we’ll be referring to a “graph” not in the sense of a visualization, but as a combination of connected nodes. A graph can be constructed from a tidy object since it has three variables:

from: the node an edge is coming from
to: the node an edge is going towards
weight: A numeric value associated with each edge

The igraph package has many powerful functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the graph_from_data_frame() function, which takes a data frame of edges with columns for “from”, “to”, and edge attributes (in this case n):
```{r}
install.packages('igraph')
library(igraph)
# original counts
bigram_counts
```

```{r}
# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph
```

We can convert an igraph object into a ggraph with the ggraph function, after which we add layers to it, much like layers are added in ggplot2. For example, for a basic graph we need to add three layers: nodes, edges, and text

```{r}
# filter for only relatively common combinations
install.packages('ggraph')
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```
we can visualize some details of the text structure. We also see pairs or triplets along the outside that form common short phrases 

We add the edge_alpha aesthetic to the link layer to make links transparent based on how common or rare the bigram is
We add directionality with an arrow, constructed using grid::arrow(), including an end_cap option that tells the arrow to end before touching the node
We tinker with the options to the node layer to make the nodes more attractive (larger, blue points)
We add a theme that’s useful for plotting networks, theme_void()
```{r}
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```
